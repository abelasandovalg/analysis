{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetic Retinopathy Debrecen Dataset - Decision Tree \n",
    "\n",
    "The goal for this analysis is to classify patients as either having or not having diabetic retinopathy. The dataset contains 1151 instances and 20 attributes (categorical and continuous) and can be found [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set).\n",
    "\n",
    "### Set up Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1151 entries, 0 to 1150\n",
      "Data columns (total 20 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   quality     1151 non-null   int64  \n",
      " 1   prescreen   1151 non-null   int64  \n",
      " 2   ma2         1151 non-null   int64  \n",
      " 3   ma3         1151 non-null   int64  \n",
      " 4   ma4         1151 non-null   int64  \n",
      " 5   ma5         1151 non-null   int64  \n",
      " 6   ma6         1151 non-null   int64  \n",
      " 7   ma7         1151 non-null   int64  \n",
      " 8   exudate8    1151 non-null   float64\n",
      " 9   exudate9    1151 non-null   float64\n",
      " 10  exudate10   1151 non-null   float64\n",
      " 11  exudate11   1151 non-null   float64\n",
      " 12  exudate12   1151 non-null   float64\n",
      " 13  exudate13   1151 non-null   float64\n",
      " 14  exudate14   1151 non-null   float64\n",
      " 15  exudate15   1151 non-null   float64\n",
      " 16  eu_dist     1151 non-null   float64\n",
      " 17  diameter    1151 non-null   float64\n",
      " 18  amfm_class  1151 non-null   int64  \n",
      " 19  label       1151 non-null   int64  \n",
      "dtypes: float64(10), int64(10)\n",
      "memory usage: 180.0 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>prescreen</th>\n",
       "      <th>ma2</th>\n",
       "      <th>ma3</th>\n",
       "      <th>ma4</th>\n",
       "      <th>ma5</th>\n",
       "      <th>ma6</th>\n",
       "      <th>ma7</th>\n",
       "      <th>exudate8</th>\n",
       "      <th>exudate9</th>\n",
       "      <th>exudate10</th>\n",
       "      <th>exudate11</th>\n",
       "      <th>exudate12</th>\n",
       "      <th>exudate13</th>\n",
       "      <th>exudate14</th>\n",
       "      <th>exudate15</th>\n",
       "      <th>eu_dist</th>\n",
       "      <th>diameter</th>\n",
       "      <th>amfm_class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>172.452792</td>\n",
       "      <td>35.357483</td>\n",
       "      <td>17.933027</td>\n",
       "      <td>5.710940</td>\n",
       "      <td>1.640322</td>\n",
       "      <td>0.574062</td>\n",
       "      <td>0.180098</td>\n",
       "      <td>0.113584</td>\n",
       "      <td>0.528910</td>\n",
       "      <td>0.121771</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>105.170765</td>\n",
       "      <td>55.359483</td>\n",
       "      <td>30.940866</td>\n",
       "      <td>7.775036</td>\n",
       "      <td>2.036927</td>\n",
       "      <td>0.865821</td>\n",
       "      <td>0.471709</td>\n",
       "      <td>0.098018</td>\n",
       "      <td>0.568779</td>\n",
       "      <td>0.106186</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>47.062703</td>\n",
       "      <td>17.400517</td>\n",
       "      <td>2.614538</td>\n",
       "      <td>0.244024</td>\n",
       "      <td>0.017430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.529220</td>\n",
       "      <td>0.108683</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>66</td>\n",
       "      <td>62</td>\n",
       "      <td>41</td>\n",
       "      <td>51.112703</td>\n",
       "      <td>13.356251</td>\n",
       "      <td>3.772144</td>\n",
       "      <td>0.255739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547192</td>\n",
       "      <td>0.082202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>128.776696</td>\n",
       "      <td>61.296220</td>\n",
       "      <td>42.974037</td>\n",
       "      <td>9.673157</td>\n",
       "      <td>1.462451</td>\n",
       "      <td>0.296022</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.485478</td>\n",
       "      <td>0.104906</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>8.818234</td>\n",
       "      <td>3.161544</td>\n",
       "      <td>1.900918</td>\n",
       "      <td>1.524727</td>\n",
       "      <td>1.292870</td>\n",
       "      <td>0.165831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538223</td>\n",
       "      <td>0.098270</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>31.004416</td>\n",
       "      <td>14.292234</td>\n",
       "      <td>0.994879</td>\n",
       "      <td>0.047469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.549666</td>\n",
       "      <td>0.133508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>145.891998</td>\n",
       "      <td>17.009009</td>\n",
       "      <td>7.400220</td>\n",
       "      <td>0.954294</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.532231</td>\n",
       "      <td>0.093025</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>145.210992</td>\n",
       "      <td>74.208340</td>\n",
       "      <td>35.536604</td>\n",
       "      <td>4.212897</td>\n",
       "      <td>0.182014</td>\n",
       "      <td>0.025564</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.524631</td>\n",
       "      <td>0.106345</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>46.832500</td>\n",
       "      <td>21.635597</td>\n",
       "      <td>2.999985</td>\n",
       "      <td>0.506774</td>\n",
       "      <td>0.124958</td>\n",
       "      <td>0.046611</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.578760</td>\n",
       "      <td>0.101157</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      quality  prescreen  ma2  ma3  ma4  ma5  ma6  ma7    exudate8   exudate9  \\\n",
       "350         1          1   44   44   42   32   23   17  172.452792  35.357483   \n",
       "697         1          1   56   50   45   39   31   23  105.170765  55.359483   \n",
       "638         1          1    6    6    6    6    5    4   47.062703  17.400517   \n",
       "194         1          1   74   73   73   66   62   41   51.112703  13.356251   \n",
       "628         1          1   24   23   20   18   16   12  128.776696  61.296220   \n",
       "22          1          0   37   34   31   30   28   24    8.818234   3.161544   \n",
       "174         1          0   13   13   13   12   11    6   31.004416  14.292234   \n",
       "118         1          1    9    9    8    7    7    4  145.891998  17.009009   \n",
       "1103        1          1    3    3    3    3    3    1  145.210992  74.208340   \n",
       "131         1          1   16   15   14   13   11    9   46.832500  21.635597   \n",
       "\n",
       "      exudate10  exudate11  exudate12  exudate13  exudate14  exudate15  \\\n",
       "350   17.933027   5.710940   1.640322   0.574062   0.180098   0.113584   \n",
       "697   30.940866   7.775036   2.036927   0.865821   0.471709   0.098018   \n",
       "638    2.614538   0.244024   0.017430   0.000000   0.000000   0.000000   \n",
       "194    3.772144   0.255739   0.000000   0.000000   0.000000   0.000000   \n",
       "628   42.974037   9.673157   1.462451   0.296022   0.003116   0.000000   \n",
       "22     1.900918   1.524727   1.292870   0.165831   0.000000   0.000000   \n",
       "174    0.994879   0.047469   0.000000   0.000000   0.000000   0.000000   \n",
       "118    7.400220   0.954294   0.001045   0.000000   0.000000   0.000000   \n",
       "1103  35.536604   4.212897   0.182014   0.025564   0.004090   0.004090   \n",
       "131    2.999985   0.506774   0.124958   0.046611   0.000992   0.000992   \n",
       "\n",
       "       eu_dist  diameter  amfm_class  label  \n",
       "350   0.528910  0.121771           1      1  \n",
       "697   0.568779  0.106186           0      1  \n",
       "638   0.529220  0.108683           0      0  \n",
       "194   0.547192  0.082202           0      0  \n",
       "628   0.485478  0.104906           1      1  \n",
       "22    0.538223  0.098270           0      1  \n",
       "174   0.549666  0.133508           0      0  \n",
       "118   0.532231  0.093025           1      1  \n",
       "1103  0.524631  0.106345           1      0  \n",
       "131   0.578760  0.101157           1      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list w column names \n",
    "col_names = []\n",
    "for i in range(20):\n",
    "    if i == 0:\n",
    "        col_names.append('quality')\n",
    "    if i == 1:\n",
    "        col_names.append('prescreen')\n",
    "    if i >= 2 and i <= 7:\n",
    "        col_names.append('ma' + str(i))\n",
    "    if i >= 8 and i <= 15:\n",
    "        col_names.append('exudate' + str(i))\n",
    "    if i == 16:\n",
    "        col_names.append('eu_dist')\n",
    "    if i == 17:\n",
    "        col_names.append('diameter')\n",
    "    if i == 18:\n",
    "        col_names.append('amfm_class')\n",
    "    if i == 19:\n",
    "        col_names.append('label')\n",
    "\n",
    "# read in data, add column names \n",
    "data = pd.read_csv(\"messidor_features.txt\", names = col_names)\n",
    "\n",
    "# preview data \n",
    "print(data.info())\n",
    "\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing \n",
    "\n",
    "Now that the data has been read in, the feature columns need to be separated from the class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151,)\n",
      "(1151, 19)\n"
     ]
    }
   ],
   "source": [
    "# separate features and labels \n",
    "labels = data['label']\n",
    "features = data.drop(['label'], axis = 1) \n",
    "\n",
    "# check shape\n",
    "print(labels.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to perform Principal Component Analysis (PCA) later one, the features need to be standardized first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = -4.1263399077018944e-17\n",
      "std = 1.0\n"
     ]
    }
   ],
   "source": [
    "# standardize data \n",
    "features_scaled = StandardScaler().fit_transform(features) \n",
    "\n",
    "# check data \n",
    "print('mean =', features_scaled.mean()) # 0 \n",
    "print('std =', features_scaled.std()) # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the standardized data needs to be split into training and testing sets for both features and labels with an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab_train = (920,) lab_test = (231,)\n",
      "feat_train = (920, 19) feat_test = (231, 19)\n"
     ]
    }
   ],
   "source": [
    "# holdout method; 80/20 split \n",
    "lab_train, lab_test, feat_train, feat_test = train_test_split(labels, features_scaled, test_size=0.20)\n",
    "\n",
    "# check split shapes \n",
    "print('lab_train =', lab_train.shape, 'lab_test =', lab_test.shape)\n",
    "print('feat_train =', feat_train.shape, 'feat_test =', feat_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA) \n",
    "\n",
    "Once split, the training set needs to be fit to the model. PCA is affected by the scale of the dataset, which is why the values were standardized previously. Once the data has been fit to the model, the explained variance ration can be extracted to calculate the cumulative sum of it and then determine how many components are needed to explain 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance in each pc: [3.34651939e-01 2.41657349e-01 1.10726399e-01 6.21819382e-02\n",
      " 5.30514477e-02 5.05400971e-02 4.29678876e-02 4.04331018e-02\n",
      " 3.09002565e-02 1.30768655e-02 7.61762625e-03 6.14258087e-03\n",
      " 2.41088746e-03 1.26111038e-03 1.08181425e-03 8.50622156e-04\n",
      " 2.62593786e-04 1.21876435e-04 6.36069516e-05]\n",
      "\n",
      "cumulative variance [0.33465194 0.57630929 0.68703569 0.74921763 0.80226907 0.85280917\n",
      " 0.89577706 0.93621016 0.96711042 0.98018728 0.98780491 0.99394749\n",
      " 0.99635838 0.99761949 0.9987013  0.99955192 0.99981452 0.99993639\n",
      " 1.        ]\n",
      "\n",
      "number of columns to keep: 9\n",
      "\n",
      "feat_train_pca: (920, 9)\n",
      "feat_test_pca: (231, 9)\n"
     ]
    }
   ],
   "source": [
    "# create and fit pca \n",
    "pca = PCA() \n",
    "pca_data = pca.fit_transform(feat_train) \n",
    "\n",
    "# explained variance ratio\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "print('variance in each pc:', var_exp) \n",
    "\n",
    "# calculate cumulative sum of var_exp\n",
    "cum_var_exp = np.cumsum(var_exp) \n",
    "print('\\ncumulative variance', cum_var_exp) \n",
    "\n",
    "# find index where cum_var_exp reaches 95%\n",
    "n = 1 + np.argmax(cum_var_exp > 0.95) \n",
    "print('\\nnumber of columns to keep:', n)\n",
    "\n",
    "# reduce training set \n",
    "feat_train_pca = pca_data[:, :n] \n",
    "\n",
    "# transform test set \n",
    "feat_test_pca = pca.transform(feat_test)[:, :n]\n",
    "\n",
    "# check data \n",
    "print('\\nfeat_train_pca:', feat_train_pca.shape) \n",
    "print('feat_test_pca:', feat_test_pca.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Now that the size of the data has been reduced using PCA, a decision tree classifier can be fit onto the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create decision tree\n",
    "tree = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "# build tree \n",
    "tree.fit(feat_train_pca, lab_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the tree with the training data, lets test the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5974025974025974\n"
     ]
    }
   ],
   "source": [
    "# classify test data \n",
    "predict = tree.predict(feat_test_pca) \n",
    "\n",
    "# calculate accuracy \n",
    "acc = accuracy_score(lab_test, predict) \n",
    "print('accuracy:', acc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model came out to 0.57, but this can be improved by modifying the parameters of the model. Let's modify the tree's max depth to see if accuracy can be improved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (max_depth=10): 0.5584415584415584\n",
      "accuracy (max_depth=15): 0.6363636363636364\n",
      "accuracy (max_depth=20): 0.6060606060606061\n"
     ]
    }
   ],
   "source": [
    "# decision trees w different max_depth\n",
    "tree_10 = DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=4)\n",
    "tree_15 = DecisionTreeClassifier(criterion='entropy', max_depth=15, random_state=4)\n",
    "tree_20 = DecisionTreeClassifier(criterion='entropy', max_depth=20, random_state=4)\n",
    "\n",
    "# build trees\n",
    "tree_10.fit(feat_train_pca, lab_train)\n",
    "tree_15.fit(feat_train_pca, lab_train)\n",
    "tree_20.fit(feat_train_pca, lab_train)\n",
    "\n",
    "# classify test data \n",
    "predict_10 = tree_10.predict(feat_test_pca) \n",
    "predict_15 = tree_15.predict(feat_test_pca) \n",
    "predict_20 = tree_20.predict(feat_test_pca) \n",
    "\n",
    "# calculate accuracy \n",
    "print('accuracy (max_depth=10):', accuracy_score(lab_test, predict_10))\n",
    "print('accuracy (max_depth=15):', accuracy_score(lab_test, predict_15))\n",
    "print('accuracy (max_depth=20):', accuracy_score(lab_test, predict_20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The holdout method, which is what was used to train and test our data, is not necessarily the best method to calculate accuracy since it can vary depending on the split and the fit of the data. The preferred method, cross-validation, will give a better indication of accuracy since it tests the model with multiple train-test splits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Cross Validation \n",
    "\n",
    "K-fold Cross Validation allows for a better estimate of accuracy. For this analysis, a 10-fold cross validation will be performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.5862069  0.60869565 0.65217391 0.63478261 0.64347826 0.64347826\n",
      " 0.62608696 0.57391304 0.67826087 0.6       ]\n",
      "\n",
      "accuracy: 0.6247076461769114\n"
     ]
    }
   ],
   "source": [
    "# create decision tree \n",
    "tree = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "# get cross validation scores\n",
    "scores = cross_val_score(tree, features_scaled, labels, cv=10)\n",
    "\n",
    "# print results\n",
    "print('scores:', scores) \n",
    "print('\\naccuracy:', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross Validation \n",
    "\n",
    "At this point, the accuracy is is still not the best since it would be incorrect about 40% of the time. Next, the model needs to be tuned in order to use the best parameters and avoid overfitting the training data. To do this, lets use Grid Search; This method will evaluate a model for each conbination of algorithm parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'max_depth': 15, 'max_features': 15, 'min_samples_leaf': 20}\n",
      "\n",
      "accuracy: 0.6490118577075099\n"
     ]
    }
   ],
   "source": [
    "# set up parameters \n",
    "params = {'max_depth': [5, 10, 15, 20], \n",
    "          'min_samples_leaf': [5, 10, 15, 20], \n",
    "          'max_features': [5, 10, 15]\n",
    "         }\n",
    "\n",
    "# create gridsearchcv \n",
    "grid_search = GridSearchCV(tree, params, cv=5, scoring='accuracy') \n",
    "\n",
    "# fit data \n",
    "grid_search.fit(features_scaled, labels)\n",
    "\n",
    "# print results \n",
    "print('best parameters:', grid_search.best_params_)\n",
    "print('\\naccuracy:', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Cross Validation \n",
    "\n",
    "Although the model has improved, it can be taken one step further by running a nested cross validation by passing GridSearchCV into cross_val_score. In doing this, the cross_val_score splits the data into train and test sets for the set number of folds and then calculates the best parameters, score, and estimator for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6263805759457933\n"
     ]
    }
   ],
   "source": [
    "# nested cross validation \n",
    "nested_score = cross_val_score(grid_search, features_scaled, labels, cv=5, scoring='accuracy')\n",
    "\n",
    "# print results \n",
    "print('accuracy:', nested_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not the best given that the model would be wrong 40% of the time. However, to improve our model in the future, some feature engineering could be done on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio",
   "language": "python",
   "name": "portfolio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
